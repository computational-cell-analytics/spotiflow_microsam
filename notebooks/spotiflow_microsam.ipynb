{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: using foundation models for segmentation and prompt automatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will introduce you to the usage of foundation models for microscopy for instance segmentation in a variety of images. Specifically, you will learn about and use _[μSAM](https://doi.org/10.1038/s41592-024-02580-4)_, a foundation model for segmentation specialized in microscopy images.\n",
    "\n",
    "- TODO: Add some images\n",
    "\n",
    "\n",
    "_μSAM_ is based on the [Segment Anything Model (SAM)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf) by Meta, which introduced the first widely-used foundational model for segmentation in natural images. SAM's backbone network is a \"simple\" Vision Transformer (ViT), but the key to its success comes from 1) its training scheme and 2) the large amount of training data (1 __billion__ masks!). Specifically, during the training phase, SAM uses a _promptable_ segmentation task which enables it to be generalistic and, at the same time, achieve excellent zero-shot capabilities. By _prompting_, we mean that the model allows user input on _what_ the user wants to segment. This prompt can come in two different flavours: _bounding boxes_ (a.k.a. draw rectangles around the object) and _point prompting_ (a.k.a. click inside the object).\n",
    "\n",
    "_μSAM_ is a specialized version of SAM that was fine-tuned on a large and diverse dataset of microscopy images. This was necessary due to the large domain gap between natural images (naturally RGB, which SAM was trained on) and microscopy images (potentially multichannel, controlled acquisitions).\n",
    "\n",
    "The fact that (μ)SAM is promptable (and the current general DL landscape :)) arises several interesting questions. Does prompting always improve performance? What is the best way to prompt it? Can we automatize prompting? We will particularly focus on the last question in this tutorial. While foundation models, like _μSAM_, are trained to be generalistic, they may still struggle in certain domains, which generally requires them to be fine-tuned to assess the _domain gap_. Nevertheless, fine-tuning such models tends to be computationally very demanding and require large amounts of resources which are not always available.\n",
    "\n",
    "In this tutorial, we will show you that another smaller, but specialized, neural network can be used to effectively prompt _μSAM_ in order to overcome the _domain gap_. In particular, we will use [_Spotiflow_](https://doi.org/10.1101/2024.02.01.578426), a neural network-based spot detection method for microscopy, which will generate _point prompts_ for _μSAM_. The usage of another smaller neural network for prompting requires it to be highly specialized and, thus, trained for each task, so it is likely that this automatic prompting strategy doesn't work right off the bat for many data modalities. Nevertheless, we encourage you to try the methods on your data and discuss the results with the TAs!\n",
    "\n",
    "\n",
    "_Authors_: Anwai Archit, Albert Dominguez Mantes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional, Literal, Tuple, List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "from skimage.measure import regionprops\n",
    "from skimage.measure import label as connected_components\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "from torch_em.data.datasets.light_microscopy.ifnuclei import get_ifnuclei_paths\n",
    "\n",
    "from spotiflow.model import Spotiflow\n",
    "\n",
    "from segment_anything import SamPredictor\n",
    "\n",
    "from micro_sam.prompt_based_segmentation import segment_from_points\n",
    "from micro_sam.util import get_sam_model, precompute_image_embeddings\n",
    "from micro_sam.automatic_segmentation import get_predictor_and_segmenter, automatic_instance_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where all required files are stored.\n",
    "ROOT = \"/scratch/denbi/k8s/ADL4IA_flexprojects/flexproject1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bacteria colony segmentation\n",
    "\n",
    "Our first task will be to detect and segment bacteria colonies in plates. We will be using the [AGAR dataset](https://agar.neurosys.com/). Let's start by visualizing a few of the images in the dataset (as one should always do!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filepaths to the input images from AGAR data.\n",
    "\n",
    "def get_agar_paths(\n",
    "    path: Union[os.PathLike, str], resolution: Optional[Literal[\"higher\", \"lower\"]] = None\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Get the filepaths to the input image and corresponding metadata file.\n",
    "\n",
    "    Args:\n",
    "        path: The folder where the input data is stored.\n",
    "        resolution: The choice of resolution.\n",
    "\n",
    "    Returns:\n",
    "        List of filepaths for the input data.\n",
    "        List of filepaths for the corresponding metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir = os.path.join(path, \"AGAR_representative\")\n",
    "\n",
    "    # Get path to one low-res image and corresponding metadata file.\n",
    "    resolution = (\"*\" if resolution is None else resolution) + \"-resolution\"\n",
    "    image_paths = sorted(glob(os.path.join(data_dir, resolution, \"*.jpg\")))\n",
    "    metadata_paths = [p.replace(\".jpg\", \".json\") for p in image_paths]\n",
    "    metadata_paths = [p for p in metadata_paths if os.path.exists(p)]\n",
    "\n",
    "    assert image_paths and len(image_paths) == len(metadata_paths)\n",
    "\n",
    "    return image_paths, metadata_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filepaths to the input images (low-resolution images from AGAR)\n",
    "image_paths, _ = get_agar_paths(path=os.path.join(ROOT, \"data\", \"agar\"), resolution=\"lower\")\n",
    "\n",
    "# Load the first few images\n",
    "images_subset = tuple(imageio.imread(p) for p in image_paths[:3])\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for ax, img in zip(axs, images_subset):\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Task</b>: Explore the data: how many images are there in the dataset? How many channels do they have?\n",
    "    There is no need to load all the images to answer these.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Question</b>: From the images that were just loaded, which challenges do you think one would face when segmenting bacteria colonies in plates?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load a Spotiflow model trained on this dataset, which we will use to generate the prompts, and run the detection on the images. The output of Spotiflow will be `y,x` coordinates corresponding, roughly, to the center of each colony. We will then visualize the detected spots on one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the folder to the Spotiflow model trained on AGAR data.\n",
    "spotiflow_model_dir = os.path.join(ROOT, \"models\", \"spotiflow\")\n",
    "\n",
    "# Load the AGAR model\n",
    "model = Spotiflow.from_folder(\n",
    "    pretrained_path=os.path.join(spotiflow_model_dir, \"spotiflow_agar\", \"agar_model\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Detect colonies as coordinates\n",
    "spots_per_image = []\n",
    "for image_path in tqdm(image_paths, desc=\"Running Spotiflow on each image\"):\n",
    "    image = imageio.imread(image_path)\n",
    "    detected_spots, _ = model.predict(image, verbose=False, min_distance=9)\n",
    "    spots_per_image.append(detected_spots)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.imshow(images_subset[0])\n",
    "ax.scatter(spots_per_image[0][:, 1], spots_per_image[0][:, 0], s=10, c=\"r\", alpha=0.5, marker=\"x\")\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the locations of the colonies, let's use them to prompt `micro-sam`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_promptable_segmentation(\n",
    "    predictor: SamPredictor, image: np.ndarray, point_prompts: List[List[Tuple[int, int]]],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the image embeddings.\n",
    "    image_embeddings = precompute_image_embeddings(\n",
    "        predictor=predictor,\n",
    "        input_=image,\n",
    "        ndim=2,  # With RGB images, we should have channels last and must set ndim to 2.\n",
    "        verbose=False,\n",
    "        # tile_shape=(384, 384),  # Tile shape for larger images.\n",
    "        # halo=(64, 64),  # Overlap shape for larger images.\n",
    "        # save_path=f\"embeddings_{i}.zarr\",  # Caches the image embeddings.\n",
    "    )\n",
    "\n",
    "    # Run promptable segmentation.\n",
    "    masks = [\n",
    "        segment_from_points(\n",
    "            predictor=predictor,\n",
    "            points=np.array([each_point_prompt]),  # Each point coordinate (Y, X) is expected as array.\n",
    "            labels=np.array([1]),  # Each corresponding label, eg. 1 corresponds positive, is expected as array.\n",
    "            image_embeddings=image_embeddings,\n",
    "        ).squeeze() for each_point_prompt in point_prompts\n",
    "    ]\n",
    "\n",
    "    # Merge all masks into one segmentation.\n",
    "    # 1. First, we get the area per object and try to map as: big objects first and small ones then\n",
    "    #    (to avoid losing tiny objects near-by or to overlaps)\n",
    "    mask_props = [{\"mask\": mask, \"area\": regionprops(connected_components(mask))[0].area} for mask in masks]\n",
    "\n",
    "    # 2. Next, we assort based on area from greatest to smallest.\n",
    "    assorted_masks = sorted(mask_props, key=(lambda x: x[\"area\"]), reverse=True)\n",
    "    masks = [per_mask[\"mask\"] for per_mask in assorted_masks]\n",
    "\n",
    "    # 3. Finally, we merge all individual segmentations into one.\n",
    "    segmentation = np.zeros(image.shape[:2], dtype=int)\n",
    "    for j, mask in enumerate(masks, start=1):\n",
    "        segmentation[mask > 0] = j\n",
    "\n",
    "    return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = False # Set to True to visualize the segmentation interactively with napari.\n",
    "\n",
    "# Get the Segment Anything Model to simulate interactive segmentation with detected spots.\n",
    "predictor = get_sam_model(\n",
    "    model_type=\"vit_b_lm\",\n",
    "    checkpoint_path=os.path.join(ROOT, \"models\", \"micro-sam\", \"vit_b_lm_v3.pt\")\n",
    ")\n",
    "\n",
    "# Run simulated interactive segmentation per image.\n",
    "for i, (image_path, point_prompts) in tqdm(\n",
    "    enumerate(zip(image_paths, spots_per_image)),\n",
    "    desc=\"Running micro-sam on each image\",\n",
    "    total=len(image_paths),\n",
    "):\n",
    "    image = imageio.imread(image_path)\n",
    "\n",
    "    segmentation = run_promptable_segmentation(predictor=predictor, image=image, point_prompts=point_prompts)\n",
    "    # Plot the first 3 images and the resulting segmentation masks\n",
    "    if i < 3:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axs[0].imshow(image)\n",
    "        axs[1].imshow(label2rgb(segmentation, image, bg_label=0))\n",
    "        axs[0].axis(\"off\")\n",
    "        axs[1].axis(\"off\")\n",
    "        axs[0].set_title(\"Image\")\n",
    "        axs[1].set_title(\"Segmentation (Spotiflow -> μSAM)\")\n",
    "\n",
    "if view:\n",
    "    # Visualize the image and corresponding segmentation (and detected spots).\n",
    "    import napari\n",
    "    v = napari.Viewer()\n",
    "    v.add_image(image)\n",
    "    v.add_labels(segmentation)\n",
    "    v.add_points(point_prompts)\n",
    "    napari.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare this with automatic segmentation of `micro-sam`, which does not require any prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run automatic instance segmentation with default parameters.\n",
    "view = False\n",
    "\n",
    "# Get the Segment Anything model and the corresponding segmentation class.\n",
    "predictor, segmenter = get_predictor_and_segmenter(\n",
    "    model_type=\"vit_b\",\n",
    "    checkpoint=os.path.join(ROOT, \"models\", \"micro-sam\", \"vit_b_lm_v3.pt\"),\n",
    "    amg=False,  # i.e. runs our new automatic instance segmentation.\n",
    "    is_tiled=False,  # overwrite if automatic segmentation is run based on tiling window\n",
    ")\n",
    "\n",
    "for i, (image_path, point_prompts) in tqdm(\n",
    "    enumerate(zip(image_paths, spots_per_image)),\n",
    "    desc=\"Running automatic segmentation with micro-sam\",\n",
    "    total=len(image_paths)\n",
    "):\n",
    "    image = imageio.imread(image_path)\n",
    "\n",
    "    # Get automatic segmentation\n",
    "    segmentation = automatic_instance_segmentation(\n",
    "        predictor=predictor,\n",
    "        segmenter=segmenter,\n",
    "        input_path=image,\n",
    "        ndim=2,\n",
    "        verbose=False,\n",
    "        # tile_shape=(384, 384),\n",
    "        # halo=(64, 64),\n",
    "    )\n",
    "    # Plot the first 3 images and the resulting segmentation masks\n",
    "    if i < 3:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axs[0].imshow(image)\n",
    "        axs[1].imshow(label2rgb(segmentation, image, bg_label=0))\n",
    "        axs[0].axis(\"off\")\n",
    "        axs[1].axis(\"off\")\n",
    "        axs[0].set_title(\"Image\")\n",
    "        axs[1].set_title(\"Segmentation (μSAM)\")\n",
    "\n",
    "if view:\n",
    "    # Visualize the last image and corresponding segmentation.\n",
    "    import napari\n",
    "    v = napari.Viewer()\n",
    "    v.add_image(image)\n",
    "    v.add_labels(segmentation)\n",
    "    napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Question</b>: In which cases does prompting help? In which cases does it not? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuclei segmentation\n",
    "\n",
    "We will now move to a different dataset containing images of nuclei, [IFNuclei](https://doi.org/10.1038/s41597-020-00608-w). We will follow the same steps as before: load the data, run Spotiflow, and prompt `micro-sam` with the detected nuclei.\n",
    "\n",
    "As a prompter, we will be using a Spotiflow model trained on the [DSB18](https://doi.org/10.1038/s41592-019-0612-7) dataset, which consists of DAPI-stained nuclei. Despite not being from the same dataset that we will be working on, the data is similar enough so that the model should be able to detect the nuclei accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Task: Use `Spotiflow`, trained on DAPI-stained images for detecting nuclei, on new fluorescence images.\n",
    "\n",
    "TODO: explain IFNuclei data\n",
    "\n",
    "TODO: explain the idea of the task - to check zero-shot performance on both tasks.\n",
    "\n",
    "TODO: elaborate on the idea that the participants are expected to establish this pipeline and try everything here onwards themselves. We can provide them some hints (eg. choice of vit model, hyperparameters in spotiflow to try, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the DSB model\n",
    "model = Spotiflow.from_folder(\n",
    "    pretrained_path=os.path.join(spotiflow_model_dir, \"spotiflow_dsb18\", \"dsb18_model\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Get DAPI-stained images.\n",
    "image_paths, gt_paths = get_ifnuclei_paths(path=os.path.join(ROOT, \"data\", \"if_nuclei\"))\n",
    "image_paths = [p for p in image_paths if os.path.basename(p).startswith(\"normal\")]  # Consider DAPI-stained images.\n",
    "gt_paths = [p for p in gt_paths if os.path.basename(p).startswith(\"normal\")]  # Consider DAPI-stained images.\n",
    "\n",
    "# Get the detected spots\n",
    "spots_per_image = []\n",
    "for image_path in tqdm(image_paths, desc=\"Running Spotiflow on each image\"):\n",
    "    image = imageio.imread(image_path)\n",
    "    detected_spots = model.predict(image, verbose=False, min_distance=9, prob_thresh=0.4)[0]\n",
    "    spots_per_image.append(detected_spots)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.imshow(image)\n",
    "ax.scatter(spots_per_image[-1][:, 1], spots_per_image[-1][:, 0], s=10, c=\"r\", alpha=0.5, marker=\"x\")\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably not what you expected... What could be happening? Well, despite the DSB18 dataset that the Spotiflow model was trained on being also DAPI-stained, several settings, _e.g._ the pixel size could be different, which could lead to the specialized model not generalizing well due to the different apparent object sizes.\n",
    "\n",
    "In any case, despite there being more detections than actual nuclei, it should not cause problems to _μSAM_ as long as the detections are inside a nuclei, as the masks will simply cancel each other out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = False\n",
    "\n",
    "# Get the Segment Anything Model to simulate interactive segmentation with detected spots.\n",
    "predictor = get_sam_model(model_type=\"vit_b_lm\", checkpoint_path=os.path.join(ROOT, \"models\", \"micro-sam\", \"vit_b_lm_v3.pt\"))\n",
    "\n",
    "# Run simulated interactive segmentation per image.\n",
    "for i, (image_path, point_prompts) in tqdm(\n",
    "    enumerate(zip(image_paths, spots_per_image)),\n",
    "    desc=\"Running micro-sam on each image\",\n",
    "    total=len(image_paths),\n",
    "):\n",
    "    image = imageio.imread(image_path)\n",
    "    \n",
    "    segmentation = run_promptable_segmentation(predictor=predictor, image=image, point_prompts=point_prompts)\n",
    "\n",
    "    # Plot the first 3 images and the resulting segmentation masks\n",
    "    if i < 3:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axs[0].imshow(image)\n",
    "        axs[1].imshow(label2rgb(segmentation, image, bg_label=0))\n",
    "        axs[0].axis(\"off\")\n",
    "        axs[1].axis(\"off\")\n",
    "        axs[0].set_title(\"Image\")\n",
    "        axs[1].set_title(\"Segmentation (Spotiflow -> μSAM)\");\n",
    "if view:\n",
    "    # Visualize the image and corresponding segmentation (and detected spots).\n",
    "    import napari\n",
    "    v = napari.Viewer()\n",
    "    v.add_image(image)\n",
    "    v.add_labels(segmentation)\n",
    "    v.add_points(point_prompts)\n",
    "    napari.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run automatic instance segmentation with default parameters.\n",
    "view = False\n",
    "\n",
    "# Get the Segment Anything model and the corresponding segmentation class.\n",
    "predictor, segmenter = get_predictor_and_segmenter(\n",
    "    model_type=\"vit_b\",\n",
    "    checkpoint=os.path.join(ROOT, \"models\", \"micro-sam\", \"vit_b_lm_v3.pt\"),\n",
    "    amg=False,  # i.e. runs our new automatic instance segmentation.\n",
    "    is_tiled=False,  # overwrite if automatic segmentation is run based on tiling window\n",
    ")\n",
    "\n",
    "for i, (image_path, point_prompts) in tqdm(\n",
    "    enumerate(zip(image_paths, spots_per_image)),\n",
    "    desc=\"Running automatic segmentation with micro-sam\",\n",
    "    total=len(image_paths)\n",
    "):\n",
    "    image = imageio.imread(image_path)\n",
    "\n",
    "    # Get automatic segmentation\n",
    "    segmentation = automatic_instance_segmentation(\n",
    "        predictor=predictor,\n",
    "        segmenter=segmenter,\n",
    "        input_path=image,\n",
    "        ndim=2,\n",
    "        verbose=False,\n",
    "        # tile_shape=(384, 384),\n",
    "        # halo=(64, 64),\n",
    "    )\n",
    "\n",
    "    # Plot the first 3 images and the resulting segmentation masks\n",
    "    if i < 3:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axs[0].imshow(image)\n",
    "        axs[1].imshow(label2rgb(segmentation, image, bg_label=0))\n",
    "        axs[0].axis(\"off\")\n",
    "        axs[1].axis(\"off\")\n",
    "        axs[0].set_title(\"Image\")\n",
    "        axs[1].set_title(\"Segmentation (μSAM)\");\n",
    "\n",
    "if view:\n",
    "    # Visualize the image and corresponding segmentation.\n",
    "    import napari\n",
    "    v = napari.Viewer()\n",
    "    v.add_image(image)\n",
    "    v.add_labels(segmentation)\n",
    "    napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Question</b>: Do you think a prompter model based on bacteria colonies would work well for nuclei segmentation? Why/why not? Feel free to try it out!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Question</b>: What would happen if a prompt point is outside the object to be segmented? Can you think of a way to prevent/postprocess the result to avoid issues?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
