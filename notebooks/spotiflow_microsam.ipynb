{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: using foundation models for segmentation and prompt automatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will introduce you to the usage of foundation models for microscopy for instance segmentation in a variety of images. Specifically, you will learn about and use _[μSAM](https://doi.org/10.1038/s41592-024-02580-4)_, a foundation model for segmentation specialized in microscopy images.\n",
    "\n",
    "- TODO: Add some images\n",
    "\n",
    "\n",
    "_μSAM_ is based on the [Segment Anything Model (SAM)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf) by Meta, which introduced the first widely-used foundational model for segmentation in natural images. SAM's backbone network is a \"simple\" Vision Transformer (ViT), but the key to its success comes from 1) its training scheme and 2) the large amount of training data (1 __billion__ masks!). Specifically, during the training phase, SAM uses a _promptable_ segmentation task which enables it to be generalistic and, at the same time, achieve excellent zero-shot capabilities. By _prompting_, we mean that the model allows user input on _what_ the user wants to segment. This prompt can come in two different flavours: _bounding boxes_ (a.k.a. draw rectangles around the object) and _point prompting_ (a.k.a. click inside the object).\n",
    "\n",
    "_μSAM_ is a specialized version of SAM that was fine-tuned on a large and diverse dataset of microscopy images. This was necessary due to the large domain gap between natural images (naturally RGB, which SAM was trained on) and microscopy images (potentially multichannel, controlled acquisitions).\n",
    "\n",
    "The fact that (μ)SAM is promptable (and the current general DL landscape :)) arises several interesting questions. Does prompting always improve performance? What is the best way to prompt it? Can we automatize prompting? We will particularly focus on the last question in this tutorial. While foundation models, like _μSAM_, are trained to be generalistic, they may still struggle in certain domains, which generally requires them to be fine-tuned to assess the _domain gap_. Nevertheless, fine-tuning such models tends to be computationally very demanding and require large amounts of resources which are not always available.\n",
    "\n",
    "In this tutorial, we will show you that another smaller, but specialized, neural network can be used to effectively prompt _μSAM_ in order to overcome the _domain gap_. In particular, we will use [_Spotiflow_](https://doi.org/10.1101/2024.02.01.578426), a neural network-based spot detection method for microscopy, which will generate _point prompts_ for _μSAM_. The usage of another smaller neural network for prompting requires it to be highly specialized and, thus, trained for each task, so it is likely that this automatic prompting strategy doesn't work right off the bat for many data modalities. Nevertheless, we encourage you to try the methods on your data and discuss the results with the TAs!\n",
    "\n",
    "\n",
    "_Authors_: Anwai Archit, Albert Dominguez Mantes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sam/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional, Literal, Tuple, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "from skimage.measure import regionprops\n",
    "from skimage.measure import label as connected_components\n",
    "\n",
    "from torch_em.data.datasets.light_microscopy.ifnuclei import get_ifnuclei_paths\n",
    "\n",
    "from spotiflow.model import Spotiflow\n",
    "\n",
    "from segment_anything import SamPredictor\n",
    "\n",
    "from micro_sam.prompt_based_segmentation import segment_from_points\n",
    "from micro_sam.util import get_sam_model, precompute_image_embeddings\n",
    "from micro_sam.automatic_segmentation import get_predictor_and_segmenter, automatic_instance_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where all required files are stored.\n",
    "ROOT = \"/scratch/denbi/k8s/ADL4IA_flexprojects/flexproject1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bacteria colony segmentation\n",
    "\n",
    "We will begin with a task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filepaths to the input images from AGAR data.\n",
    "\n",
    "def get_agar_paths(\n",
    "    path: Union[os.PathLike, str], resolution: Optional[Literal[\"higher\", \"lower\"]] = None\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Get the filepaths to the input image and corresponding metadata file.\n",
    "\n",
    "    Args:\n",
    "        path: The folder where the input data is stored.\n",
    "        resolution: The choice of resolution.\n",
    "\n",
    "    Returns:\n",
    "        List of filepaths for the input data.\n",
    "        List of filepaths for the corresponding metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir = os.path.join(path, \"AGAR_representative\")\n",
    "\n",
    "    # Get path to one low-res image and corresponding metadata file.\n",
    "    resolution = (\"*\" if resolution is None else resolution) + \"-resolution\"\n",
    "    image_paths = sorted(glob(os.path.join(data_dir, resolution, \"*.jpg\")))\n",
    "    metadata_paths = [p.replace(\".jpg\", \".json\") for p in image_paths]\n",
    "    metadata_paths = [p for p in metadata_paths if os.path.exists(p)]\n",
    "\n",
    "    assert image_paths and len(image_paths) == len(metadata_paths)\n",
    "\n",
    "    return image_paths, metadata_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the folder to the Spotiflow model trained on AGAR data.\n",
    "spotiflow_model_dir = os.path.join(ROOT, \"models\", \"spotiflow\")\n",
    "\n",
    "# Get the filepaths to the input images (low-resolution images from AGAR)\n",
    "image_paths, _ = get_agar_paths(path=os.path.join(ROOT, \"data\", \"agar\"), resolution=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:spotiflow.model.spotiflow:Loading model from folder: /scratch/denbi/k8s/ADL4IA_flexprojects/flexproject1/models/spotiflow/spotiflow_agar/agar_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spotiflow.model.spotiflow:Loading model from folder: /scratch/denbi/k8s/ADL4IA_flexprojects/flexproject1/models/spotiflow/spotiflow_agar/agar_model\n",
      "Running Spotiflow on each image: 100%|██████████| 10/10 [00:03<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run the AGAR model\n",
    "model = Spotiflow.from_folder(\n",
    "    pretrained_path=os.path.join(spotiflow_model_dir, \"spotiflow_agar\", \"agar_model\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Get the detected spots\n",
    "spots_per_image = []\n",
    "for image_path in tqdm(image_paths, desc=\"Running Spotiflow on each image\"):\n",
    "    image = imageio.imread(image_path)\n",
    "    detected_spots = model.predict(image, verbose=False, min_distance=9)[0]\n",
    "    spots_per_image.append(detected_spots)\n",
    "\n",
    "    # TODO: visualize spots for 1 image only (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `micro-sam` with the detected spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_promptable_segmentation(\n",
    "    predictor: SamPredictor, image: np.ndarray, point_prompts: List[List[Tuple[int, int]]],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the image embeddings.\n",
    "    image_embeddings = precompute_image_embeddings(\n",
    "        predictor=predictor,\n",
    "        input_=image,\n",
    "        ndim=2,  # With RGB images, we should have channels last and must set ndim to 2.\n",
    "        verbose=False,\n",
    "        # tile_shape=(384, 384),  # Tile shape for larger images.\n",
    "        # halo=(64, 64),  # Overlap shape for larger images.\n",
    "        # save_path=f\"embeddings_{i}.zarr\",  # Caches the image embeddings.\n",
    "    )\n",
    "\n",
    "    # Run promptable segmentation.\n",
    "    masks = [\n",
    "        segment_from_points(\n",
    "            predictor=predictor,\n",
    "            points=np.array([each_point_prompt]),  # Each point coordinate (Y, X) is expected as array.\n",
    "            labels=np.array([1]),  # Each corresponding label, eg. 1 corresponds positive, is expected as array.\n",
    "            image_embeddings=image_embeddings,\n",
    "        ).squeeze() for each_point_prompt in point_prompts\n",
    "    ]\n",
    "\n",
    "    # Merge all masks into one segmentation.\n",
    "    # 1. First, we get the area per object and try to map as: big objects first and small ones then\n",
    "    #    (to avoid losing tiny objects near-by or to overlaps)\n",
    "    mask_props = [{\"mask\": mask, \"area\": regionprops(connected_components(mask))[0].area} for mask in masks]\n",
    "\n",
    "    # 2. Next, we assort based on area from greatest to smallest.\n",
    "    assorted_masks = sorted(mask_props, key=(lambda x: x[\"area\"]), reverse=True)\n",
    "    masks = [per_mask[\"mask\"] for per_mask in assorted_masks]\n",
    "\n",
    "    # 3. Finally, we merge all individual segmentations into one.\n",
    "    segmentation = np.zeros(image.shape[:2], dtype=int)\n",
    "    for j, mask in enumerate(masks, start=1):\n",
    "        segmentation[mask > 0] = j\n",
    "\n",
    "    return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running micro-sam on each image: 100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "view = False\n",
    "\n",
    "# Get the Segment Anything Model to simulate interactive segmentation with detected spots.\n",
    "predictor = get_sam_model(\n",
    "    model_type=\"vit_b_lm\",\n",
    "    checkpoint_path=os.path.join(ROOT, \"models\", \"micro-sam\", \"vit_b_lm_v3.pt\")\n",
    ")\n",
    "\n",
    "# Run simulated interactive segmentation per image.\n",
    "for i, (image_path, point_prompts) in tqdm(\n",
    "    enumerate(zip(image_paths, spots_per_image)),\n",
    "    desc=\"Running micro-sam on each image\",\n",
    "    total=len(image_paths),\n",
    "):\n",
    "    image = imageio.imread(image_path)\n",
    "\n",
    "    segmentation = run_promptable_segmentation(predictor=predictor, image=image, point_prompts=point_prompts)\n",
    "\n",
    "    if view:\n",
    "        # Visualize the image and corresponding segmentation (and detected spots).\n",
    "        import napari\n",
    "        v = napari.Viewer()\n",
    "        v.add_image(image)\n",
    "        v.add_labels(segmentation)\n",
    "        v.add_points(point_prompts)\n",
    "        napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with automatic segmentation of `micro-sam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running automatic segmentation with micro-sam: 100%|██████████| 10/10 [00:16<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run automatic instance segmentation with default parameters.\n",
    "view = False\n",
    "\n",
    "# Get the Segment Anything model and the corresponding segmentation class.\n",
    "predictor, segmenter = get_predictor_and_segmenter(\n",
    "    model_type=\"vit_b\",\n",
    "    checkpoint=os.path.join(ROOT, \"models\", \"micro-sam\", \"vit_b_lm_v3.pt\"),\n",
    "    amg=False,  # i.e. runs our new automatic instance segmentation.\n",
    "    is_tiled=False,  # overwrite if automatic segmentation is run based on tiling window\n",
    ")\n",
    "\n",
    "for i, (image_path, point_prompts) in tqdm(\n",
    "    enumerate(zip(image_paths, spots_per_image)),\n",
    "    desc=\"Running automatic segmentation with micro-sam\",\n",
    "    total=len(image_paths)\n",
    "):\n",
    "    image = imageio.imread(image_path)\n",
    "\n",
    "    # Get automatic segmentation\n",
    "    segmentation = automatic_instance_segmentation(\n",
    "        predictor=predictor,\n",
    "        segmenter=segmenter,\n",
    "        input_path=image,\n",
    "        ndim=2,\n",
    "        verbose=False,\n",
    "        # tile_shape=(384, 384),\n",
    "        # halo=(64, 64),\n",
    "    )\n",
    "\n",
    "    if view:\n",
    "        # Visualize the image and corresponding segmentation.\n",
    "        import napari\n",
    "        v = napari.Viewer()\n",
    "        v.add_image(image)\n",
    "        v.add_labels(segmentation)\n",
    "        napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Task: Use `Spotiflow`, trained on DAPI-stained images for detecting nuclei, on new fluorescence images.\n",
    "\n",
    "TODO: explain IFNuclei data\n",
    "\n",
    "TODO: explain the idea of the task - to check zero-shot performance on both tasks.\n",
    "\n",
    "TODO: elaborate on the idea that the participants are expected to establish this pipeline and try everything here onwards themselves. We can provide them some hints (eg. choice of vit model, hyperparameters in spotiflow to try, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:spotiflow.model.spotiflow:Loading model from folder: /scratch/denbi/k8s/ADL4IA_flexprojects/flexproject1/models/spotiflow/spotiflow_dsb18/dsb18_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spotiflow.model.spotiflow:Loading model from folder: /scratch/denbi/k8s/ADL4IA_flexprojects/flexproject1/models/spotiflow/spotiflow_dsb18/dsb18_model\n",
      "Running Spotiflow on each image: 100%|██████████| 41/41 [00:04<00:00, 10.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run the DSB model\n",
    "model = Spotiflow.from_folder(\n",
    "    pretrained_path=os.path.join(spotiflow_model_dir, \"spotiflow_dsb18\", \"dsb18_model\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Get DAPI-stained images.\n",
    "image_paths, gt_paths = get_ifnuclei_paths(path=os.path.join(ROOT, \"data\", \"if_nuclei\"))\n",
    "image_paths = [p for p in image_paths if os.path.basename(p).startswith(\"normal\")]  # Consider DAPI-stained images.\n",
    "gt_paths = [p for p in gt_paths if os.path.basename(p).startswith(\"normal\")]  # Consider DAPI-stained images.\n",
    "\n",
    "# Get the detected spots\n",
    "spots_per_image = []\n",
    "for image_path in tqdm(image_paths, desc=\"Running Spotiflow on each image\"):\n",
    "    image = imageio.imread(image_path)\n",
    "    detected_spots = model.predict(image, verbose=False, min_distance=9, prob_thresh=0.4)[0]\n",
    "    spots_per_image.append(detected_spots)\n",
    "\n",
    "    # TODO: visualize spots for 1 image only (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running micro-sam on each image:   0%|          | 0/41 [00:00<?, ?it/s]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:   2%|▏         | 1/41 [00:02<01:37,  2.45s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:   5%|▍         | 2/41 [00:05<01:42,  2.62s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:   7%|▋         | 3/41 [00:06<01:14,  1.97s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  10%|▉         | 4/41 [00:08<01:15,  2.03s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  12%|█▏        | 5/41 [00:10<01:12,  2.02s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  15%|█▍        | 6/41 [00:11<01:00,  1.74s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  17%|█▋        | 7/41 [00:14<01:05,  1.93s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  20%|█▉        | 8/41 [00:16<01:13,  2.23s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  22%|██▏       | 9/41 [00:18<01:07,  2.12s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  24%|██▍       | 10/41 [00:25<01:45,  3.41s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  27%|██▋       | 11/41 [00:27<01:33,  3.10s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  29%|██▉       | 12/41 [00:30<01:24,  2.93s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  32%|███▏      | 13/41 [00:31<01:11,  2.54s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  34%|███▍      | 14/41 [00:32<00:58,  2.16s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  37%|███▋      | 15/41 [00:34<00:50,  1.96s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  39%|███▉      | 16/41 [00:35<00:43,  1.75s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  41%|████▏     | 17/41 [00:38<00:46,  1.93s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  44%|████▍     | 18/41 [00:39<00:40,  1.78s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  46%|████▋     | 19/41 [00:42<00:46,  2.12s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  49%|████▉     | 20/41 [00:45<00:52,  2.51s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  51%|█████     | 21/41 [00:48<00:52,  2.60s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  54%|█████▎    | 22/41 [00:51<00:50,  2.66s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  56%|█████▌    | 23/41 [00:53<00:45,  2.53s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  59%|█████▊    | 24/41 [00:55<00:41,  2.42s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  61%|██████    | 25/41 [00:59<00:45,  2.82s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  63%|██████▎   | 26/41 [01:03<00:45,  3.03s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  66%|██████▌   | 27/41 [01:05<00:41,  2.96s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  68%|██████▊   | 28/41 [01:08<00:35,  2.76s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  71%|███████   | 29/41 [01:10<00:33,  2.77s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  73%|███████▎  | 30/41 [01:12<00:27,  2.51s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  76%|███████▌  | 31/41 [01:19<00:36,  3.62s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  78%|███████▊  | 32/41 [01:21<00:30,  3.36s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  80%|████████  | 33/41 [01:25<00:27,  3.38s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  83%|████████▎ | 34/41 [01:28<00:22,  3.23s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  85%|████████▌ | 35/41 [01:38<00:32,  5.49s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  88%|████████▊ | 36/41 [01:41<00:23,  4.61s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  90%|█████████ | 37/41 [01:43<00:15,  3.92s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  93%|█████████▎| 38/41 [01:46<00:10,  3.53s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  95%|█████████▌| 39/41 [01:49<00:06,  3.37s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image:  98%|█████████▊| 40/41 [01:51<00:03,  3.12s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running micro-sam on each image: 100%|██████████| 41/41 [01:54<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "view = False\n",
    "\n",
    "# Get the Segment Anything Model to simulate interactive segmentation with detected spots.\n",
    "predictor = get_sam_model(model_type=\"vit_b_lm\", checkpoint_path=os.path.join(ROOT, \"models\", \"micro-sam\", \"vit_b_lm_v3.pt\"))\n",
    "\n",
    "# Run simulated interactive segmentation per image.\n",
    "for i, (image_path, point_prompts) in tqdm(\n",
    "    enumerate(zip(image_paths, spots_per_image)),\n",
    "    desc=\"Running micro-sam on each image\",\n",
    "    total=len(image_paths),\n",
    "):\n",
    "    image = imageio.imread(image_path)\n",
    "    \n",
    "    segmentation = run_promptable_segmentation(predictor=predictor, image=image, point_prompts=point_prompts)\n",
    "\n",
    "    if view:\n",
    "        # Visualize the image and corresponding segmentation (and detected spots).\n",
    "        import napari\n",
    "        v = napari.Viewer()\n",
    "        v.add_image(image)\n",
    "        v.add_labels(segmentation)\n",
    "        v.add_points(point_prompts)\n",
    "        napari.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running automatic segmentation with micro-sam:   0%|          | 0/41 [00:00<?, ?it/s]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:   2%|▏         | 1/41 [00:01<01:08,  1.72s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:   5%|▍         | 2/41 [00:02<00:54,  1.40s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:   7%|▋         | 3/41 [00:04<00:54,  1.44s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  10%|▉         | 4/41 [00:06<00:57,  1.55s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  12%|█▏        | 5/41 [00:07<00:51,  1.42s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  15%|█▍        | 6/41 [00:09<00:55,  1.59s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  17%|█▋        | 7/41 [00:10<00:49,  1.46s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  20%|█▉        | 8/41 [00:11<00:46,  1.40s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  22%|██▏       | 9/41 [00:13<00:46,  1.45s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  24%|██▍       | 10/41 [00:14<00:43,  1.41s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  27%|██▋       | 11/41 [00:15<00:41,  1.38s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  29%|██▉       | 12/41 [00:17<00:39,  1.38s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  32%|███▏      | 13/41 [00:18<00:38,  1.38s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  34%|███▍      | 14/41 [00:19<00:36,  1.37s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  37%|███▋      | 15/41 [00:21<00:35,  1.36s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  39%|███▉      | 16/41 [00:26<01:06,  2.66s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  41%|████▏     | 17/41 [00:28<00:53,  2.24s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  44%|████▍     | 18/41 [00:29<00:46,  2.01s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  46%|████▋     | 19/41 [00:32<00:46,  2.11s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  49%|████▉     | 20/41 [00:33<00:40,  1.94s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  51%|█████     | 21/41 [00:35<00:36,  1.82s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  54%|█████▎    | 22/41 [00:37<00:35,  1.86s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  56%|█████▌    | 23/41 [00:38<00:32,  1.82s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  59%|█████▊    | 24/41 [00:40<00:29,  1.74s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  61%|██████    | 25/41 [00:42<00:31,  1.96s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  63%|██████▎   | 26/41 [00:44<00:28,  1.89s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  66%|██████▌   | 27/41 [00:46<00:28,  2.03s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  68%|██████▊   | 28/41 [00:48<00:25,  1.92s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  71%|███████   | 29/41 [00:51<00:25,  2.14s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  73%|███████▎  | 30/41 [00:53<00:23,  2.17s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  76%|███████▌  | 31/41 [00:55<00:20,  2.06s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  78%|███████▊  | 32/41 [00:57<00:19,  2.14s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  80%|████████  | 33/41 [00:59<00:16,  2.02s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  83%|████████▎ | 34/41 [01:01<00:15,  2.19s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  85%|████████▌ | 35/41 [01:04<00:13,  2.27s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  88%|████████▊ | 36/41 [01:06<00:10,  2.18s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  90%|█████████ | 37/41 [01:08<00:08,  2.15s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  93%|█████████▎| 38/41 [01:10<00:06,  2.10s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  95%|█████████▌| 39/41 [01:12<00:04,  2.09s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam:  98%|█████████▊| 40/41 [01:15<00:02,  2.28s/it]WARNING: could not determine DPI\n",
      "WARNING:vispy:could not determine DPI\n",
      "Running automatic segmentation with micro-sam: 100%|██████████| 41/41 [01:17<00:00,  1.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run automatic instance segmentation with default parameters.\n",
    "view = False\n",
    "\n",
    "# Get the Segment Anything model and the corresponding segmentation class.\n",
    "predictor, segmenter = get_predictor_and_segmenter(\n",
    "    model_type=\"vit_b\",\n",
    "    checkpoint=os.path.join(ROOT, \"models\", \"micro-sam\", \"vit_b_lm_v3.pt\"),\n",
    "    amg=False,  # i.e. runs our new automatic instance segmentation.\n",
    "    is_tiled=False,  # overwrite if automatic segmentation is run based on tiling window\n",
    ")\n",
    "\n",
    "for i, (image_path, point_prompts) in tqdm(\n",
    "    enumerate(zip(image_paths, spots_per_image)),\n",
    "    desc=\"Running automatic segmentation with micro-sam\",\n",
    "    total=len(image_paths)\n",
    "):\n",
    "    image = imageio.imread(image_path)\n",
    "\n",
    "    # Get automatic segmentation\n",
    "    segmentation = automatic_instance_segmentation(\n",
    "        predictor=predictor,\n",
    "        segmenter=segmenter,\n",
    "        input_path=image,\n",
    "        ndim=2,\n",
    "        verbose=False,\n",
    "        # tile_shape=(384, 384),\n",
    "        # halo=(64, 64),\n",
    "    )\n",
    "\n",
    "    if view:\n",
    "        # Visualize the image and corresponding segmentation.\n",
    "        import napari\n",
    "        v = napari.Viewer()\n",
    "        v.add_image(image)\n",
    "        v.add_labels(segmentation)\n",
    "        napari.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
